先简单从宏观上认识一下机器学习：
	机器学习的产生，其实是帮助程序员解决那些无法靠编写具体规则/逻辑来实现的问题，这其中的计算超出了人类意识理解范畴。
	_机器学习_（machine learning，ML）是一类强大的可以从经验中学习的技术。 通常采用观测数据或与环境交互的形式，机器学习算法会积累更多的经验，其性能也会逐步提高。

**机器学习和深度学习的关系？** 
两者是从属关系，深度学习是机器学习的子集。
机器学习的核心是从数据中提取特征，构建模型进行预测或决策。常见方法：线性回归、决策树、支持向量机(SVM)等。
而深度学习是机器学习的子领域，核心是使用**深层神经网络**（多隐藏层的神经网络）自动学习数据的多层次特征。  典型模型：卷积神经网络(CNN)、循环神经网络(RNN)、Transformer等。

| **方法**    | **传统机器学习**           | **深度学习**           |
| --------- | -------------------- | ------------------ |
| **特征**    | 依赖人工设计特征（如图像中的边缘、纹理） | 自动学习多层次特征（从简单到复杂）  |
| **数据需求**  | 小规模数据即可训练            | 需要海量数据和强大算力（如GPU）  |
| **模型复杂度** | 相对简单（参数少）            | 高度复杂（百万至千亿级参数）     |
| **可解释性**  | 模型结果较易解释（如决策树规则）     | 黑箱模型，可解释性差         |
| **应用场景**  | 结构化数据（表格、文本分类）       | 非结构化数据（图像、语音、自然语言） |
### 1 机器学习的组成
无论什么类型的机器学习问题，基本都包含以下几个部分：
1. 可以用来学习的 **数据（data）**；
2. 如何转换数据的 **模型（model）**；
3. **目标函数（objective function）**，用来量化模型的有效性；
4. 调整模型参数以优化目标函数的 **算法（algorithm）**。
简单来说机器学习就是用数据训练模型的过程：
1. 从一个随机初始化参数的模型开始，这个模型基本没有“智能”；
2. 获取一些数据样本（例如，音频片段以及对应的是或否标签）；
3. 调整参数，使模型在这些样本中表现得更好；
4. 重复第（2）步和第（3）步，直到模型在任务中的表现令人满意。
![[Pasted image 20250220095521.png]]
> 总之，要有这样一个思想：我们不是在编写一个唤醒词识别器，而是编写了一个“学习”程序。如果我们把一个庞大的带标签的数据集喂给它，他很有可能能够“学习”识别某种类型的数据。
> 这种“通过用数据集来确定程序行为”的方法可以被看作用数据编程。

#### 1.1 数据
每个**数据集**由一个个 **样本**(example, sample)组成，它们遵循独立同分布。每个样本由一组称为 **特征**(features)的属性组成。机器学习模型会根据这些属性进行预测，要预测的目标属性，被称为 **标签**。
当每个样本的特征类别数量都是相同的时候，即其特征向量是固定长度的，这个长度被称为数据的 **维数**(dimensionality)。 固定长度的特征向量是很方便的，这可以用来量化学习样本。但并不是所有数据都可以用“固定长度”的向量表示，比如图像数据可能分辨率不同；文本数据就更是如此。
与传统机器学习方法相比，深度学习的一个主要优势是可以处理不同长度的数据。

一般而言，更多地数据可以被用来训练更强的模型。而事实是，一些深度学习模型在小数据集上能够工作，但其效能并不比传统方法高。此外，仅仅拥有海量的数据是不够的，我们还需要正确的数据。如果数据中充满了错误，或者如果数据的特征不能预测任务目标，那么模型很可能无效。

#### 1.2 模型
模型代表了输入-输出的映射关系；其输出由许多`参数`(parameter)决定，然后使用数据集来确定当下的`“最佳参数集”`，这些参数通过某种性能度量方式来达到完成任务的最佳性能。任一调整参数后的程序被称为`模型`（model）。
深度学习与经典方法的区别主要在于：前者关注的功能强大的模型，这些模型由神经网络错综复杂的交织在一起，包含层层数据转换，因此被称为 深度学习 (deep learning)。 在讨论深度模型的过程中，本书也将提及一些传统方法。

#### 1.3 目标函数
所谓“学习”，是指自主提高模型完成某些任务的效能。但什么才算真正的提高呢？
在机器学习中，我们需要有一个**模型的优劣程度的度量**，这个度量在大多数情况是“可优化”的，这被称之为`目标函数/损失函数`（objective function）。
这里有一些例子：
	在试图预测数值时，最常见的损失函数是`平方误差`，即预测值与实际值之差的平方。解决分类问题时，最常见的目标函数是`最小化错误率`，即预测与实际情况不符的样本比例。
通常，损失函数是根据模型参数定义的，并取决于数据集。可用数据集通常可以分成两部分：训练数据集用于拟合模型参数，测试数据集用于评估拟合的模型。然后我们观察模型在这两部分数据集的性能。
当一个模型在训练集上表现良好，但不能推广到测试集时，这个模型被称为`过拟合`（overfitting）的。 就像在现实生活中，尽管模拟考试考得很好，真正的考试不一定百发百中。
#### 1.4 优化算法
当我们确定了数据集、模型和一个合适的损失函数，接下来就需要一种算法，它能够搜索出`最佳参数`，以最小化损失函数。深度学习中，大多流行的优化算法通常基于一种基本方法–`**梯度下降**`（gradient descent）：在每个步骤中，梯度下降法都会检查每个参数，看看如果仅对该参数进行少量变动，训练集损失会朝哪个方向移动，并在减少损失的方向上优化参数。

### 2 各种机器学习问题
下面将列出一些常见的机器学习问题和应用，为之后本书的讨论做铺垫
#### 2.1 监督学习
**监督学习**(supervised learning)擅长在“给定输入特征”的情况下**预测标签**。 每个“特征-标签”对都称为一个样本(example)。 有时，即使标签是未知的，样本也可以指代输入特征。 目标是生成一个模型，能够将任何输入特征映射到标签（即**预测**）。

监督学习在训练参数时，为模型提供了一个数据集，其中每个样本都有真实的标签。通过训练，使模型能够在给定一组特定的可用数据的情况下，估计未知事物的概率。比如：根据计算机断层扫描（CT）肿瘤图像，预测是否为癌症；给出一个英语句子，预测正确的法语翻译；根据本月的财务报告数据，预测下个月股票的价格。

监督学习的学习过程一般可以分为三大步骤：
1. 从已知大量数据样本中随机选取一个子集，为每个样本获取真实标签。有时，这些样本已有标签；有时，这些样本可能需要被人工标记（例如，图像分类）。这些**输入和相应的标签**一起构成了训练数据集；
2. 选择有监督的学习算法，它将训练数据集作为输入，并输出一个“已完成学习的模型”；
3. 将之前没有见过的样本特征放到这个“已完成学习的模型”中，使用模型的输出作为相应标签的预测。
 ![[Pasted image 20250220105437.png]]
 
##### 2.1.1回归
_回归_（regression）是最简单的监督学习任务之一。当标签取任意数值时，我们称之为 _回归_ 问题，此时的目标是生成一个模型，使它的预测非常接近实际标签值。
总而言之，判断回归问题的一个很好的经验法则是，**任何有关“有多少”的问题很可能就是回归问题。** 比如：预测房屋价格，预测未来的降雨量等。
事实上，我们在生活中许多问题就是采用简单的线性回归算法进行计算的。
>回归的本质是训练一个回归函数来输出一个数值

##### 2.1.2 分类
而有的问题更希望回答“哪一个”，这类问题叫做 _分类_（classification）问题。 _分类_ 问题希望模型能够预测样本属于哪个 _类别_（category，正式称为 _类_（class））。比如，**手写字符识别/图像识别**；其本质都是分类问题。
>分类是训练一个**分类器**来输出预测的类别。

最简单的分类问题，比如回答“是”或“不是”；这被称之为 _二项分类_。
当有两个以上的类别时，我们把这个问题称为 _多项分类_ 问题。 常见的例子包括手写字符识别` {0,1,2,...9,a,b,c,...}`。 
与解决回归问题不同，分类问题的常见损失函数被称为 _交叉熵_（cross-entropy）

**注意：** 在实际中，最常见的类别不一定是最终用于决策的类别。比如我们训练了一个毒蘑菇检测分类器，哪怕分类器80%确定图中的蘑菇不是毒蘑菇；我们也不会输出食用的决策，因为不确定风险的影响远远大于收益。这时，我们需要将“**预期风险**”作为损失函数，即需要将结果的概率乘以与之相关的收益（或伤害）。 假设，食用蘑菇造成的损失为`0.2×∞+0.8×0=∞`，而丢弃蘑菇的损失为`0.2×0+0.8×1=0.8`。谨慎，是有道理的。

##### 2.1.3 标记问题
当我们想让模型描绘输入图像的内容，比如一只猫、一只公鸡、一只狗，还有一头驴。这类学习预测不相互排斥的类别的问题称为 _多标签分类_（multi-label classification）。

##### 2.1.4 搜索
简单来说就是对相关性进行检索，并对检索结果进行排序。

##### 2.1.5 推荐系统
另一类与搜索和排名相关的问题是 _推荐系统_（recommender system），它的目标是向特定用户进行“个性化”推荐。

##### 2.1.6 序列学习
以上大多数问题都具有固定大小的输入和产生固定大小的输出，输入的样本之间没有任何关系；但是如果输入是连续的，模型可能就需要拥有“记忆”功能。比如处理视频片段时。
序列学习需要摄取输入序列或预测输出序列，或两者兼而有之。 具体来说，输入和输出都是可变长度的序列，例如机器翻译和从语音中转录文本。 虽然不可能考虑所有类型的序列转换，但以下特殊情况值得一提：标记和解析、自动语音识别、文本到语音、机器翻译。

#### 2.2 无监督学习
到目前为止，所有的例子都与监督学习有关，即需要向模型提供巨大数据集：每个样本包含特征和相应标签值。
“监督学习”模型像一个打工仔，有一份极其专业的工作和一位极其平庸的老板。 老板站在身后，准确地告诉模型在每种情况下应该做什么，直到模型学会从情况到行动的映射。
相反，如果工作没有十分具体的目标，就需要“自发”地去学习了。 比如，老板可能会给我们一大堆数据，然后要求用它做一些数据科学研究，却没有对结果有要求。 这类数据中不含有“目标”的机器学习问题通常被为 _无监督学习_（unsupervised learning）， 本书后面的章节将讨论无监督学习技术。下面给出一些例子：
- _聚类_（clustering）问题：没有标签的情况下，我们是否能给数据分类呢？比如，给定一组照片，我们能把它们分成风景照片、狗、婴儿、猫和山峰的照片吗？同样，给定一组用户的网页浏览记录，我们能否将具有相似行为的用户聚类呢？
- _主成分分析_（principal component analysis）问题：我们能否找到少量的参数来准确地捕捉数据的线性相关属性？比如，一个球的运动轨迹可以用球的速度、直径和质量来描述。再比如，裁缝们已经开发出了一小部分参数，这些参数相当准确地描述了人体的形状，以适应衣服的需要。
- _因果关系_（causality）和 _概率图模型_（probabilistic graphical models）问题：我们能否描述观察到的许多数据的根本原因？例如，如果我们有关于房价、污染、犯罪、地理位置、教育和工资的人口统计数据，我们能否简单地根据经验数据发现它们之间的关系？
- _生成对抗性网络_（generative adversarial networks）：为我们提供一种合成数据的方法，甚至像图像和音频这样复杂的非结构化数据。潜在的统计机制是检查真实和虚假数据是否相同的测试，它是无监督学习的另一个重要而令人兴奋的领域。
#### 2.3 与环境互动
到目前为止，不管是监督学习还是无监督学习，我们都会预先获取大量数据，然后启动模型，不与环境交互。 这里所有学习都是在算法与环境断开后进行的，被称为 _离线学习_（offline learning）。
这种简单的离线学习有它的魅力。 好的一面是，我们可以孤立地进行模式识别，而不必分心于其他问题。 但缺点是，解决的问题相当有限。
此外，由于训练和测试数据不同时，最后一个问题提出了 _分布偏移_（distribution shift）的问题。 
接下来的内容将简要描述强化学习问题，这是一类明确考虑与环境交互的问题。

#### 2.4 强化学习
如果你对使用机器学习开发与环境交互并采取行动感兴趣，那么最终可能会专注于 _强化学习_(reinforcement learning)。 
这可能包括应用到**机器人、对话系统，甚至开发视频游戏的人工智能（AI）**。 _深度强化学习_ 将深度学习应用于强化学习的问题，是非常热门的研究领域。 突破性的深度 _Q网络_ 在雅达利游戏中仅使用视觉输入就击败了人类， 以及 AlphaGo 程序在棋盘游戏围棋中击败了世界冠军，是两个突出强化学习的例子。

在强化学习问题中，智能体（agent）在一系列的时间步骤上与环境交互。 在每个特定时间点，智能体从环境接收一些 _观察_（observation），并且必须选择一个 _动作_（action），然后通过某种机制（有时称为执行器）将其传输回环境，最后智能体从环境中获得 _奖励_（reward）。 此后新一轮循环开始，智能体接收后续观察，并选择后续操作，依此类推。
![[Pasted image 20250220144041.png]]
注意，强化学习的目标是产生一个好的 _策略_（policy）。 强化学习智能体选择的“动作”受策略控制，即一个从环境观察映射到行动的功能。

强化学习框架的**通用性**十分强大。 例如，我们可以将任何监督学习问题转化为强化学习问题。 假设我们有一个分类问题，可以创建一个强化学习智能体，每个分类对应一个“动作”。 然后，我们可以创建一个环境，该环境给予智能体的奖励。 这个奖励与原始监督学习问题的损失函数是一致的。
当环境可被完全观察到时，强化学习问题被称为 _马尔可夫决策过程_（markov decision process）。 当状态不依赖于之前的操作时，我们称该问题为 _上下文赌博机_（contextual bandit problem）。 当没有状态，只有一组最初未知回报的可用动作时，这个问题就是经典的 _多臂赌博机_（multi-armed bandit problem）。