作为机器学习科学家，我们的目标是建立一个具有泛化性的模型，而不是简单地记住了数据。模型需要应用于未知的数据。

我们将模型在**训练数据**上拟合的比在潜在分布中更接近的现象称为 _过拟合_（overfitting），用于对抗过拟合的技术称为 _正则化_（regularization）。
在实验中调整模型架构或超参数时会发现： 如果有足够多的神经元、层数和训练迭代周期， 模型最终可以在训练集上达到完美的精度，此时测试集的准确性却下降了；这本就是矛盾的。

### 训练误差和泛化误差
为了进一步讨论这一现象，我们需要了解训练误差和泛化误差。
	训练误差 = 你在练习题本上做错的题目
	泛化误差 = 期末考试遇到新题时做错的题目
实际中，我们用测试集来估计泛化误差

当我们训练模型时，我们试图找到一个能够尽可能拟合训练数据的函数。 但是如果它执行地“太好了”，而不能对看不见的数据做到很好泛化，就会导致过拟合。 这种情况正是我们想要避免或控制的。 深度学习中有许多启发式的技术旨在防止过拟合。
### 统计学习理论
我们假设训练数据和测试数据都是从相同的分布中独立提取的。 这通常被称为 _独立同分布假设_（i.i.d. assumption）， 这意味着对数据进行采样的过程没有进行“记忆”。 换句话说，抽取的第2个样本和第3个样本的相关性， 并不比抽取的第2个样本和第200万个样本的相关性更强。

### 模型复杂性+模型选择
模型复杂性由什么构成？一个模型是否能很好地泛化取决于很多因素， 例如参数数量，迭代次数。
为了给出一些直观的印象，我们将重点介绍几个倾向于影响模型泛化的因素。
1. 可调整参数的数量。当可调整参数的数量（有时称为 _自由度_）很大时，模型往往更容易过拟合。
2. 参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。
3. 训练样本的数量。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。

我们通常在评估几个候选模型后选择最终的模型，这个过程叫做 _模型选择_。为了确定候选模型中的最佳模型，我们通常会使用**验证集**。
>**注意：此时还没有使用到测试集！！！**

#### 验证集
原则上，在我们确定所有的超参数之前，我们不希望用到测试集。
常见做法是将我们的数据分成三份， 除了训练和测试数据集之外，还增加一个 _验证数据集_（validation dataset）， 也叫 _验证集_（validation set）。

 但现实是验证数据和测试数据之间的边界模糊得令人担忧。 除非另有明确说明，否则在这本书的实验中， 我们实际上是在使用应该被正确地称为训练数据和验证数据的数据集， 并没有真正的测试数据集。

#### K折交叉验证
当训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个合适的验证集。
解决方案是采用𝐾折交叉验证。 这里，原始训练数据被分成𝐾个不重叠的子集。 然后执行𝐾次模型训练和验证，每次在𝐾−1个子集上进行训练， 并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。 最后，通过对𝐾次实验的结果取平均来估计训练和验证误差。

###  欠拟合还是过拟合？
这里分两种情况来说：
	1. 训练误差和验证误差都很严重，但相差不大。如果模型不能降低训练误差，这可能意味着模型过于简单（表达能力不足）。此外，由于训练和验证误差之间的很小，我们有理由相信可以用一个更复杂的模型降低训练误差。 这种现象被称为_欠拟合_（underfitting）。
	2. 当我们的训练误差明显低于验证误差时要小心， 这表明严重的 _过拟合_（overfitting）。但是，我们通常更关心验证误差，而不是训练误差和验证误差之间的差距。
是否过拟合或欠拟合可能取决于**模型复杂性**和**可用训练数据集**的大小， 这两个点将在下面进行讨论。
#### 模型复杂度
以多项式拟合模型为例：
$$
y = \sum x^{i}w_{i}
$$
 事实上，当数据样本包含了𝑥的不同值时， 函数阶数等于数据样本数量的多项式函数可以完美拟合训练集。 我们直观地描述了多项式的阶数和欠拟合与过拟合之间的关系。
 ![[Pasted image 20250312103901.png | 400]]

#### 数据集大小
另一个重要因素是数据集的大小。 训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合。 随着训练数据量的增加，泛化误差通常会减小。



