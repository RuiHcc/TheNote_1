本章中，我们将第一次介绍真正的 **_深度_ 网络**。
最简单的深度网络称为 **_多层感知机_** 。多层感知机由多层神经元组成， 每一层与它的上一层相连，从中接收输入； 同时每一层也与它的下一层相连，影响当前层的神经元。

当我们训练容量较大的模型时，我们面临着_过拟合_的风险。 因此，本章将从基本的概念介绍开始讲起，包括 _过拟合_、_欠拟合_ 和模型选择。为了解决这些问题，本章将介绍 **_权重衰减_ 和 _暂退法_** 等正则化技术。 我们还将讨论数值稳定性和参数初始化相关的问题， 这些问题是成功训练深度网络的关键。

最后给了一个案例：房价预测


### 隐藏层
在softmax分类模型中，通过单个仿射变换将输入直接映射到输出，然后进行softmax操作。

但实际上，这种线性模型可能会出错，而且我们难以通过简单的预处理来解决这个问题。这是因为任何像素的重要性都以复杂的方式取决于该像素的上下文（周围像素的值）。所以我们需要一种方式来表示特征之间的交互作用，由此引入了**隐藏层**

我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型。 要做到这一点，最简单的方法是将许多全连接层堆叠在一起。 每一层都输出到上面的层，直到生成最后的输出。
我们可以把前𝐿−1层看作表示，把最后一层看作线性预测器。 
这种架构称为 **_多层感知机_** （multilayer perceptron）

下图所示的多层感知机，由于其是全连接的，所以其本质上和之前的线性模型没有什么区别。
也就是说，全连接层的多层感知机没有任何意义！
![[Pasted image 20250310105655.png | 500]]
（看做两层仿射函数的话，仿射函数的仿射函数仍是仿射函数）
$$
\begin{matrix}
H = XW^{(1)} + b^{(1)} \\
O = HW^{(2)} + b^{(2)} \\
O = XW^{(1)}W^{(2)} + b^{(1)}W^{(2)} + b^{(2)} = WX + b
\end{matrix} 
$$我们还需要一个额外的关键要素： 在仿射变换之后对每个隐藏单元应用非线性的 **_激活函数_**（activation function）𝜎。 激活函数的输出（例如，𝜎(⋅)）被称为 **_活性值_**（activations）。 有了激活函数，就不可能再将我们的多层感知机退化成线性模型：
$$
\begin{matrix}
H = \sigma (XW^{(1)} + b^{(1)}) \\
O = HW^{(2)} + b^{(2)} \\
\end{matrix} 
$$


### 激活函数：
**_激活函数_**（activation function）通过计算来确定神经元是否应该被激活；大多数激活函数都是非线性的。激活函数是深度学习的基础，下面简要介绍一些常见的激活函数：
#### ReLU函数
$$
ReLU(x) = max(x, 0)
$$
![[Pasted image 20250310111827.png | 600]]
使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。 这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题（稍后将详细介绍）。

#### sigmoid函数
**_sigmoid函数_** 将输入变换为区间(0, 1)上的输出
$$
sigmoid(x) = 1/(1+exp(-x))
$$
![[Pasted image 20250310204728.png]]
#### tanh函数
$$
tanh(x) = (1-exp(-2x))/(1+exp(-2x))
$$
![[Pasted image 20250310204823.png]]

小结：
- 多层感知机在输出层和输入层之间增加一个或多个全连接隐藏层，并通过激活函数转换隐藏层的输出。超参数为：隐藏层数、和每个隐藏层大小
- 常用的激活函数包括ReLU函数、sigmoid函数和tanh函数。