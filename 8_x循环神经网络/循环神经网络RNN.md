### RNN模型
**结构**
![[Pasted image 20250505143439.png | 800]]
更新隐藏状态：$h_t=\phi(W_{hh}h_{t-1} + W_{hx}x_{t-1} + b_h)$
输出：$o_t =W_{ho}h_t+b_o$
	如果去掉更新隐藏状态$h_t$ 这一步RNN就退化成MLP
 
**举个例子**：观测到“你” 预测输出“好”
![[Pasted image 20250505143847.png | 500]]
#### 困惑度（perplexity）
**衡量一个语言模型**的好坏可以用平均交叉熵：

$$
π = \frac{1}{n} \sum_{i=1}^n -log P(x_t|x_{t-1},...)
$$
$p$是语言模型的预测概率（softmax)，$x_t$ 是真实词
历史原因NLP使用困惑度$exp(π)$来衡量，是平均每次可能选项
	1表示完美，无穷大是最差情况

#### 梯度裁剪

迭代中计算这T个时间步上的梯度，在反向传播过程中产生长度为O(T)的矩阵乘法链，导致数值不稳定
梯度裁剪能够有效预防梯度爆炸
	eg：如果梯度长度超过$\theta$ ，那么拖影回长度 $\theta$ 
$$
g = min(1, \frac{\theta}{||g||})g
$$
 
#### RNNs 应用：
![[Pasted image 20250505144444.png | 600]]

# rnn通过时间反向传播
梯度截断对于确保模型收敛至关重要。 为了更好地理解此问题，本节将回顾序列模型梯度的计算方式；循环神经网络中的前向传播相对简单。 _通过时间反向传播_（backpropagation through time，BPTT） ([Werbos, 1990](https://zh-v2.d2l.ai/chapter_references/zreferences.html#id182 "Werbos, P. J. (1990). Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10), 1550–1560."))实际上是循环神经网络中反向传播技术的一个特定应用。
我们将循环神经网络的计算图一次展开一个时间步， 以获得模型变量和参数之间的依赖关系。 然后，基于链式法则，应用反向传播来计算和存储梯度。 由于序列可能相当长，因此依赖关系也可能相当长。 例如，某个1000个字符的序列， 其第一个词元可能会对最后位置的词元产生重大影响。 这在计算上是不可行的（它需要的时间和内存都太多了）， 并且还需要超过1000个矩阵的乘积才能得到非常难以捉摸的梯度。 这个过程充满了计算与统计的不确定性。

- “通过时间反向传播”仅仅适用于反向传播在具有隐状态的序列模型。
- 截断是计算方便性和数值稳定性的需要。截断包括：规则截断和随机截断。
- 矩阵的高次幂可能导致神经网络特征值的发散或消失，将以梯度爆炸或梯度消失的形式表现。
- 为了计算的效率，“通过时间反向传播”在计算期间会缓存中间值。