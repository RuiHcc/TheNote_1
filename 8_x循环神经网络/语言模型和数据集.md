**NLP**：**N**atuarl **L**anguage **P**rocessing 自然语言处理


假设长度为𝑇的文本序列中的词元依次为𝑥1,𝑥2,…,𝑥𝑇。
那么，𝑥𝑡（1≤𝑡≤𝑇） 可以被认为是**文本序列**在时间步𝑡处的观测或标签。 在给定这样的文本序列时，_语言模型_（language model）的目标是**估计序列的联合概率**
$$
P(x_1, x_2, ..., x_T)
$$
当需要抽取一个词元𝑥𝑡~$P(x_t| x_{t-1}, ..., x_1)$ ， 一个理想的语言模型就能够基于模型本身生成自然文本。相当于基于前面的对话片断中的文本， 就足以生成一个有意义的对话。
应用包括：
	做预训练模型：GPT，BERT
	生成文本
	判断多个序列中哪一个更常见


#### 学习语言模型
显而易见，我们面对的问题是如何对一个文档， 甚至是一个词元序列进行建模。
假设在单词级别对文本数据进行词元化， 我们可以依靠在 [8.1节](https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/sequence.html#sec-sequence)中对序列模型的分析。 让我们从基本概率规则开始：
$$P(x1, ..., x_T) = \prod_{t=1}^T P(x_t|x_{t-1},...,x_1) $$
例如，包含了四个单词的一个文本序列的概率是：
$$
P(deep, learning, is, fun) = P(deep)P(learning|deep)P(is|deep, learning)
P(fun|deep, learning, is)$$
为了训练语言模型，我们需要计算单词的概率， 以及给定前面几个单词后出现某个单词的条件概率。 **这些概率本质上就是语言模型的参数。**

#### 用计数来建模：
假设我们的训练数据集是一个大型的文本语料库：比如wiki百科；训练数据集中词的概率可以根据给定词的**相对词频**来计算。那么，
$$
P^{ \^ }(learning|deep)= \frac{n(deep,learning)}{n(deep)}
$$
其中𝑛(𝑥)和𝑛(𝑥,𝑥′)分别是单个单词和连续单词对的出现次数。但是，一般而言连续单词对“deep learning”的出现频率要低得多，这样估计无异于大海捞针。而对于三个或者更多的单词组合，情况会变得更糟。 许多合理的三个单词组合可能是存在的，但是在数据集中却找不到。

一种常见的策略是执行某种形式的 _拉普拉斯平滑_（Laplace smoothing）， 具体方法是在所有计数中添加一个小常量。 用𝑛表示训练集中的单词总数，用𝑚表示唯一单词的数量。 此解决方案有助于处理单元素问题，例如通过：
$$
\begin{matrix}
P^{ \^ }(x)= \frac{n(x)+𝜖_1/m}{n+𝜖_1} \\
P^{ \^ }(x'|x)= \frac{n(x,x')+𝜖_2P(x')} {n(x)+𝜖_2} \\
P^{ \^ }(x'|x)= \frac{n(x,x',x'')+𝜖_3P(x'')} {n(x,x')+𝜖_3}
\end{matrix}
$$
其中，𝜖1,𝜖2和𝜖3是超参数。 以𝜖1为例：当𝜖1=0时，不应用平滑； 当𝜖1接近正无穷大时，𝑃^(𝑥)接近均匀概率分布1/𝑚。
然而，这样的模型很容易变得无效，原因如下： 首先，我们需要存储所有的计数； 其次，这完全忽略了单词的意思。 例如，“猫”（cat）和“猫科动物”（feline）可能出现在相关的上下文中， 但是想根据上下文调整这类模型其实是相当困难的。 最后，长单词序列大部分是没出现过的， 因此一个模型如果只是简单地统计先前“看到”的单词序列频率， 那么模型面对这种问题肯定是表现不佳的。

#### 马尔可夫模型与𝑛元语法
当序列特别**长**的时候，采用马尔科夫假设缓解这个问题
在讨论包含深度学习的解决方案之前，我们需要了解更多的概念和术语。 回想一下我们在 [8.1节](https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/sequence.html#sec-sequence)中对马尔可夫模型的讨论， 并且将其应用于语言建模。 
如果𝑃(𝑥𝑡+1∣𝑥𝑡,…,𝑥1)=𝑃(𝑥𝑡+1∣𝑥𝑡)， 则序列上的分布满足一阶**马尔可夫性质**。 阶数越高，对应的依赖关系就越长。 
这种性质推导出了许多可以应用于序列建模的近似公式：
一元语法：
$$
P(x1, x2, x3, x4) = P(x1)P(x2)P(x3)P(x4)
$$
二元语法：
$$
\begin{matrix}
P(x1, x2, x3, x4) = P(x1)P(x2|x1)P(x3|x2)P(x4|x3)\\
=\frac{n(x1)}{n} \frac{n(x1,x2)}{n(x1)} 
\frac{n(x2,x3)}{n(x2)} \frac{n(x3,x4)}{n(x3)}
\end{matrix}
$$
通常，涉及一个、两个和三个变量的概率公式分别被称为 _一元语法_（unigram）、_二元语法_（bigram）和 _三元语法_（trigram）模型。 下面，我们将学习如何去设计更好的模型。

这能够很大程度上缓解计算复杂度 $0(n)$
#### 自然语言统计
做n元语法时


- 语言模型是自然语言处理的关键。
- 𝑛元语法通过截断相关性，为处理长序列提供了一种实用的模型。
- 长序列存在一个问题：它们很少出现或者从不出现。
- 齐普夫定律支配着单词的分布，这个分布不仅适用于一元语法，还适用于其他𝑛元语法。
- 通过拉普拉斯平滑法可以有效地处理结构丰富而频率不足的低频词词组。
- 读取长序列的主要方式是随机采样和顺序分区。在迭代过程中，后者可以保证来自两个相邻的小批量中的子序列在原始序列上也是相邻的。