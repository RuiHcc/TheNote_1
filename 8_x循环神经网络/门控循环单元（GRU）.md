#### some problems:
**说白了简单的rnn没法处理太长的序列**
而且不是每个观察值都是同等重要

- 我们可能会遇到这样的情况：早期观测值对预测所有未来观测值具有非常重要的意义。 
	考虑一个极端情况，其中第一个观测值包含一个校验和， 目标是在序列的末尾辨别校验和是否正确。 在这种情况下，**第一个词元的影响至关重要**。
我们希望有某些机制能够在一个记忆元里存储重要的早期信息。

- 我们可能会遇到这样的情况：一些词元没有相关的观测值。 
	例如，在对网页内容进行情感分析时， 可能有一些辅助HTML代码与网页传达的情绪无关。 我们希望有一些机制来_跳过_隐状态表示中的此类词元。

- 我们可能会遇到这样的情况：序列的各个部分之间存在逻辑中断。 
	例如，书的章节之间可能会有过渡存在， 或者证券的熊市和牛市之间可能会有过渡存在。 在这种情况下，最好有一种方法来 _重置_ 我们的内部状态表示。

### 门控隐状态
门控循环单元与普通的循环神经网络之间的关键区别在于： 前者**支持隐状态的门控**。
这意味着模型有专门的机制来确定应该何时更新隐状态， 以及应该何时重置隐状态。 这些机制是可学习的，并且能够解决了上面列出的问题。
	例如，如果第一个词元非常重要， 模型将学会在第一次观测之后不更新隐状态。 同样，模型也可以学会跳过不相关的临时观测。 最后，模型还将学会在需要的时候重置隐状态。 下面我们将详细讨论各类门控。

#### 重置门和更新门
由于我们想要只记住相关的观察需要：
- 能关注的机制 （更新门）
- 能遗忘的机制 （重置门）
**输入**是由当前时间步的输入和前一时间步的隐状态给出。 两个门的**输出**是由使用sigmoid激活函数的两个全连接层给出。
![[gru-1.svg | 500]]
**数学表达:**
$Rt$  和 $Zt$ 是和$Ht$相同形状的张量；
$Xt$$（样本个数n，输入个数d)$；$Ht-1$上一时间步的隐状态$（n，隐藏单元个数h）$
$$
\begin{matrix}
Rt = \sigma (X_tW{xr} + H_{t-1}W_{hr} + b_r) \\
Zt = \sigma (X_tW{xz} + H_{t-1}W_{hz} + b_z)
\end{matrix}
$$
我们使用sigmoid函数（如 [4.1节](https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/mlp.html#sec-mlp)中介绍的） 将输入值转换到区间(0,1)。

#### **候选隐状态**

![[gru-2.svg | 500]]
将重置门𝑅𝑡 与 [(8.4.5)](https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/rnn.html#equation-rnn-h-with-state) 中的常规隐状态更新机制集成， 得到在时间步𝑡的 **_候选隐状态_** （candidate hidden state） 𝐻~𝑡∈𝑅𝑛×ℎ。
$$
Ht' = tanh(X_tW_{xh} + (R_t⊙H_{t-1})W_{hh} + b_h)
$$
符号⊙是Hadamard积（按元素乘积）运算符。 在这里，我们使用tanh非线性激活函数来确保候选隐状态中的值保持在区间(−1,1)中。

#### 总结
简单来说就是**选择**让隐状态多去看 **过去的状态** 还是更关注 **现在的状态**
![[gru-3.svg | 500]]
候选隐状态，我们仍然需要结合更新门𝑍𝑡的效果。 这一步确定新的隐状态𝐻𝑡∈𝑅𝑛×ℎ 在多大程度上来自旧的状态𝐻𝑡−1和 新的候选状态𝐻~𝑡。
$$
Ht = Zt⊙Ht-1 + (1-Zt)⊙Ht'
$$
每当更新门𝑍𝑡接近1时，模型就倾向只保留旧状态。此时，来自𝑋𝑡的观测基本上被忽略， 从而有效地跳过了依赖链条中的时间步𝑡。 相反，当𝑍𝑡接近0时， 新的隐状态𝐻𝑡就会接近候选隐状态𝐻~𝑡。

总之，门控循环单元具有以下两个显著特征：
- 重置门$Rt$ 有助于捕获序列中的短期依赖关系；
- 更新门$Zt$ 有助于捕获序列中的长期依赖关系。

这些设计可以帮助我们处理循环神经网络中的梯度消失问题， 并更好地捕获时间步距离很长的序列的依赖关系。