之前，我们往往是在给定前面观测的情况下，对下一个输出进行预测，但有时根据前面的输入，我们没法给出合适的预测。例如：下面的文本填空
- 我`___`。
- 我`___`饿了。
- 我`___`饿了，我可以吃半头猪。
很显然，**后面的信息更加关键**

#### 双向模型：
如果我们希望在循环神经网络中拥有一种机制， 使之能够提供**前瞻能力**， 我们就需要修改循环神经网络的设计。
需要增加一个“从最后一个词元开始从后向前运行”的循环神经网络， 而不是只有一个在前向模式下“从第一个词元开始运行”的循环神经网络。 
**_双向循环神经网络_（bidirectional RNNs）** 添加了反向传递信息的隐藏层，以便更灵活地处理此类信息。 [图9.4.2](https://zh-v2.d2l.ai/chapter_recurrent-modern/bi-rnn.html#fig-birnn)描述了具有单个隐藏层的双向循环神经网络的架构。
![[birnn.svg#pic_center | 650]]

 双向循环神经网络没有这样容易理解的解释， 我们只能把它们当作通用的、可学习的函数。 这种转变集中体现了现代深度网络的设计原则： 首先使用经典统计模型的函数依赖类型，然后将其参数化为通用形式。

**双向循环神经网络的定义**：
 在双向架构中，我们设该时间步的前向和反向隐状态分别为 𝐻→𝑡∈𝑅𝑛×ℎ和 𝐻←𝑡∈𝑅𝑛×ℎ， 其中ℎ是隐藏单元的数目。 前向和反向隐状态的更新如下：
 $$
 \begin{matrix}
 𝐻→𝑡 = \phi(XtW_{xh}^f + 𝐻→_{𝑡-1}W_{hh}^f + b_h^f) \\ 
 𝐻←𝑡 = \phi(X_tW_{xh}^b + 𝐻←_{𝑡-1}W_{hh}^b + b_h^b)
\end{matrix}
 $$
 接下来，将**前向隐状态𝐻→𝑡** 和**反向隐状态𝐻←𝑡**连接起来， 获得需要送入输出层的隐状态𝐻𝑡∈𝑅𝑛×2ℎ。 在具有多个隐藏层的深度双向循环神经网络中， 该信息作为输入传递到下一个双向层。 最后，输出层计算得到的输出为 𝑂𝑡∈𝑅𝑛×𝑞（𝑞是输出单元的数目）：
 $$
 O_t = H_tW_{hq} + b_q
 $$
**事实上，这两个方向可以拥有不同数量的隐藏单元。**


双向循环神经网络的一个关键特性是：使用来自序列两端的信息来估计输出。 也就是说，我们使用来自过去和未来的观测信息来预测当前的观测。

但是在对下一个词元进行预测的情况中，这样的模型并不是我们所需的。
另一个严重问题是，双向循环神经网络的计算速度非常慢。

双向层的使用在实践中非常少，并且仅仅应用于部分场合。

- 在双向循环神经网络中，每个时间步的隐状态由当前时间步的前后数据同时决定。
- 双向循环神经网络与概率图模型中的“前向-后向”算法具有相似性。
- 双向循环神经网络主要用于序列编码和给定双向上下文的观测估计。
- 由于梯度链更长，因此双向循环神经网络的训练代价非常高。