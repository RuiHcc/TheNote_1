**“注意力”的由来：**
灵长类动物的视觉系统接受了大量的感官输入， 这些感官输入远远超过了大脑能够完全处理的程度。 然而，并非所有刺激的影响都是相等的。 意识的聚集和专注使灵长类动物能够在复杂的视觉环境中将注意力引向感兴趣的物体，例如猎物和天敌。 只关注一小部分信息的能力对进化更加有意义，使人类得以生存和成功。

### 查询、键和值
- “是否包含 自主性提示”将注意力机制和全连接层或汇聚层区别开来。
- 注意力机制则显示地考虑自主性提示：
	- **自主性提示被称为查询（query）**
	- 给定任意查询q 注意力机制通过`注意力汇聚（attention pooling）` 将选择引导至 `感官输入（sensory input）` ，例如中间特征表示。
	- 在注意力机制中，这些感官输入被称为 **_值_（value）**。 更通俗的解释，每个值v都与一个 _键_（key）配对， 这可以想象为感官输入的**非自主提示**。
	- 通过注意力池化层来有偏向性的选择某些输入：可以通过设计注意力汇聚的方式， 便于给定的查询（自主性提示）与键（非自主性提示）进行匹配， 这将引导得出最匹配的值（感官输入）。
![[qkv.svg#center_pic|650]]
**注意力机制通过注意力汇聚将查询q(自主性提示) 和 键k 结合在一起，实现对值v 的选择倾向**

### 注意力汇聚：Nadaraya-Watson回归
[[qkv.svg]]介绍了注意力框架下的主要组件：q和k之间的交互形成了注意力汇聚；注意力汇聚有选择性地汇聚了值v(感官输入)，以生成最终的输出。

#### **非参注意力汇聚：** 不需要学习任何东西
- 给定数据$(xi,yi),i=1,...,n$
- 平均池化层是最简单的方案：$f(x)=\frac{1}{n} \sum yi就是query$ 
- 最好的方案是60年代提出的N-W核回归：
- ![[Pasted image 20250509100742.png#pic_center|450]]
- 这里使用高斯核$$[[K(u)=\frac1{\sqrt{2π}}exp(-\frac{u^2}{2})]]$$ 那么$$\begin{matrix} 
f(x)=\sum \frac{exp(-0.5*(x-x_i)^2)} 
{\sum exp(-0.5(x-x_j)^2)} y_i \\ 
=\sum softmax(-0.5(x-x_i)^2)y_i
 \end{matrix}$$
 **下面对上述公式进行解释：** 如果一个键值$x_i$ 越接近给定的查询$x$ 那么分配给这个键的对应值$y_i$的注意力权重就会越大，就“获得了更多的注意力”。
  所以，更加**通用的注意力汇聚公式**为：
  $$
  f(x)=\sum \alpha(x,x_i)y_i$$
 
#### **带参数的注意力汇聚：在此基础上引入可以学习的$w$**
 - $$f(x)=\sum softmax(-0.5((x-x_i)w)^2)y_i$$
### 注意力评分函数：
上面使用了高斯核来对q和k之间的关系进行建模：其中**高斯核的指数部分**可以视为**注意力评价函数**
注意力汇聚的输出就是基于注意力权重的值的加权和。
![[attention-output.svg|650]]
用数学语言描述，
$$
f(q,(k1,v1),...,(km,vm)) = \sum \alpha(q,ki)vi
$$
其中，注意力权重（标量）是通过注意力评分函数$a$经过$softmax$操作得到的。

接下来介绍两个相对流行的评分函数：
1、加性注意力 `additive attention`
当q和k是不同长度的向量时：
$$
a(q,k) = w_v^T tanh(W_qq + W_kk)
$$

2、点积注意力 `scaled dot-product attention`
当q和k具有相同长度d时：
$$
a(q,k) = q^T k/\sqrt{d}
$$
