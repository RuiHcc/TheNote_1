回顾一下机器翻译的过程：
	一个基于两个循环神经网络的编码器-解码器架构 -> 循环神经网络`编码器`将长度可变的序列转换为固定形状的**上下文变量**， 然后循环神经网络`解码器`根据生成的词元和上下文变量 按词元生成输出（目标）序列词元。
但是，**并非所有输入（源）词元都对解码某个词元都有用**， 在每个解码步骤中仍使用编码_相同_的上下文变量  ->  从直觉上来讲这会影响翻译的效果。
我们希望模型将仅对齐（或参与）输入序列中与当前预测相关的部分。通过将上下文变量视为注意力集中的输出来实现的。
所以，我们像通过注意力机制来解决这个问题

### 模型：
下面描述的`Bahdanau注意力模型`：时间步𝑡′的上下文变量$c_{t'}$ ，假设输入序列中有𝑇个词元， 解码时间步𝑡′的上下文变量是注意力集中的输出：
$$
c_{t'} = \sum_{t=1}^{T} \alpha(s_{t'-1},h_t)h_t
$$
时间步𝑡′−1时的解码器隐状态**𝑠𝑡′−1是查询**， 编码器隐状态**ℎ𝑡既是键，也是值**， 注意力权重𝛼是使用 [(10.3.2)](https://zh-v2.d2l.ai/chapter_attention-mechanisms/attention-scoring-functions.html#equation-eq-attn-scoring-alpha) 所定义的加性注意力打分函数计算的。
**一个带有Bahdanau注意力的循环神经网络编码器-解码器模型:**
![[seq2seq-attention-details.svg|650]]

**关键点：**
- 编码器encoder不变，解码器decode引入注意力机制
- 编码器在所有时间步的最终层隐状态，将作为注意力的`键`和`值`
- 上一时间步的编码器全层隐状态，作为初始化解码器的隐状态
- 编码器的有效长度
- 在每个解码时间步中，解码器上一时间步的最终层隐状态作为`查询`，因此注意力输出和输入嵌入embedding都作为循环神经网络解码器的输入