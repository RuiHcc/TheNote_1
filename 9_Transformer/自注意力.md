### **多头注意力：**
当给定查询q，键k和值v的集合后，我们希望模型可以基于相同的注意力机制（比如可加性）学习不同的行为，并将不同的行为作为知识组合起来。以此捕获不同序列之间各种范围的关系。
**具体做法**：用独立学习得到的x组不同的`线性投影`来变换查询、键和值。然后x组变换后的qkv并行送到（相同）`注意力汇聚`中；最后将x个注意力汇聚的输出`连接`起来，再通过一个`线性投影`，生成最终输出。

对于x个注意力汇聚输出，每一个注意力汇聚都被称作一个 _头_（head）。 [图10.5.1](https://zh-v2.d2l.ai/chapter_attention-mechanisms/multihead-attention.html#fig-multi-head-attention) 展示了使用全连接层来实现可学习的线性变换的多头注意力。
![[multi-head-attention.svg|650]]
**数学模型：**
给定查询𝑞、 键𝑘和 值𝑣， 每个注意力头ℎ𝑖（𝑖=1,…,ℎ）的计算方法为：
$$
h_i = f(W_i^qq, W_i^kk, W_i^vv)∈R^{p_v}
$$
其中，可学习的参数包括 𝑊𝑖(𝑞)、 𝑊𝑖(𝑘)和 𝑊𝑖(𝑣)， 以及代表注意力汇聚的函数𝑓。 𝑓可以是 [10.3节](https://zh-v2.d2l.ai/chapter_attention-mechanisms/attention-scoring-functions.html#sec-attention-scoring-functions)中的 加性注意力和缩放点积注意力。

多头注意力的输出需要经过另一个线性转换， 它对应着ℎ个头连结后的结果，因此其可学习参数是 𝑊𝑜：
$$
Wo\left[
 \begin{matrix}
   h1 \\
   ...  \\
   h_x  \\
  \end{matrix} 
\right]
$$
基于这种设计，每个头都可能会关注输入的不同部分， 可以表示比简单加权平均值更复杂的函数。

### **自注意力：**
**自注意力和传统注意力的区别**：
- 传统的注意力机制通常用于处理两个不同序列之间的关系，比如机器翻译中的源语言和目标语言。
- 而自注意力是同一个序列内部元素之间的注意力，用于捕捉内部依赖关系。例如，在Transformer中，自注意力让每个词可以关注到同一句子中的其他词，从而更好地理解上下文。

| ​**特征** | ​​**传统注意力**               | **自注意力（Self-Attention）​**  |
| ------- | ------------------------- | -------------------------- |
| 作用对象    | 两个不同序列（源序列<->目标序列）        | 同一序列内部元素的关系                |
| 典型应用场景  | Seq2Seq模型（如机器翻译）          | Transformer、BERT等自回归/自编码模型 |
| 目的      | 对齐不同序列的对应位置               | 捕捉**同一序列内部的全局依赖关系**        |
| 计算方法    | Query来自解码器，Key/Value来自编码器 | Query、Key、Value均来自同一输入序列   |

**跟CNN，RNN对比：**


|       | CNN        | RNN       | 自注意力      |
| ----- | ---------- | --------- | --------- |
| 计算复杂度 | $O(knd^2)$ | $O(nd^2)$ | $O(n^2d)$ |
| 并行度   | O(n)       | O(1)      | O(n)      |
| 最长路径  | O(n/k)     | O(n)      | O(1)      |

**位置编码：**
在处理词元序列时，循环神经网络是逐个的重复地处理词元的， 而自注意力则因为并行计算而放弃了顺序操作。 为了使用序列的顺序信息，通过在输入表示中添加 _位置编码_（positional encoding）来注入绝对的或相对的位置信息。

**基于正弦函数和余弦函数的固定位置编码**：
假设输入表示𝑋∈𝑅𝑛×𝑑 包含一个序列中𝑛个词元的𝑑维嵌入表示。 位置编码使用相同形状的位置嵌入矩阵 𝑃∈𝑅𝑛×𝑑输出𝑋+𝑃， 矩阵第𝑖行、第2𝑗列和2𝑗+1列上的元素为：
$$
\begin{matrix}
p_{i,2j} = sin(\frac{i}{10000^{2j/d}}) \\
p_{i,2j+1} = cos(\frac{i}{10000^{2j/d}}) \\
\end{matrix}
$$
在位置嵌入矩阵𝑃中， 行代表词元在序列中的位置，列代表位置编码的不同维度；通过频率和偏移量来区别不同列。
