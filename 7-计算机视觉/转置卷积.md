卷积不会增大输入的高宽，通常要么不变，要么减半
转置卷积则可以用来增大输入的高宽
![[Pasted image 20250421104604.png]]
模拟的卷积操作的**反向过程**
转置卷积不是卷积的逆操作，（不可逆）**值**完全变了  

#### 为什么称之为“转置”
[什么是deconv操作（反卷积，转置卷积） - 大师兄啊哈 - 博客园 (cnblogs.com)](https://www.cnblogs.com/harrymore/p/17254833.html)

**看上面的链接吧，下面的过于抽象**
对于卷积`Y=X*W`
	可以对`W`构造一个`V`，使得卷积等价于矩阵乘法`Y' = VX'`
	这里的Y'，X' 是Y,X对应的向量版本
转置卷积则等价于`Y' = VX'`
如果卷积将输入从`(h,w)` 变成了 `(h',w')`
	同样超参数的转置卷积则从`(h',w')` 变成 `(h,w)`

转置卷积对形状的改变，与卷积对形状的改变相反：
![[Pasted image 20250421111254.png]]
feature_map确实变小了，但通道数也增加，信息并没有减少

转置卷积不是用来还原原来图像的RGB值的

转置卷积是一种卷积：
	它将输入和核进行了重新排列
	同卷积做下采样不同，它通常用作上采样

例子：当填充为0步幅为1时：
（**这里的计算还不是很懂，但可以先关注转置卷积带来的形状的变换**）
	将输入填充k-1（k是核窗口）
	将核矩阵上下、左右翻转
	然后做正常卷积
	![[Pasted image 20250421153125.png | 600]]

 形状换算：**转置卷积 和 卷积 形状上是逆变换**
	 输入高（宽）为n，核k，填充p，步幅s
	 **转置卷积**：`n' = sn + k - 2p - s`
		 回看卷积：`n' = (n-k+2p+s)/s `  --> `n = sn' +k-2p-s ` 这样就和上面对应起来了
		如果想让高宽成倍增加，可令`k  = 2p+s`

同反卷积的关系：
	数学上的反卷积（deconvolution）是指卷积的逆运算（**不同**）
	反卷积很少用在深度学习当中
		我们说的反卷积神经网络指的是转置卷积神经网络