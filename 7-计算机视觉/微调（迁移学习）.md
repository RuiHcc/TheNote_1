我们希望我们在很大数据集上训练的模型，能够帮助我们应用在自己不那么大的数据集上实现很好的效果，这里就引入了迁移学习的概念，也称**微调**

一般一个神经网络的架构，我们可以看成两块：
	一部分是在进行特征抽取，将原始像素变成容易线性分割的特征
	另一部分是在分类，线性分类器（softmax回归）
那么微调的思想就是，既然我通过IMAG-net训练的数据集，也能够帮助我在自己的数据集上提取特征，只是最后的分类标号可能不同了
![[Pasted image 20250409191043.png | 500]]

微调包括以下四个步骤： 
	1. 在源数据集（例如ImageNet数据集）上预训练神经网络模型，即 _源模型_。
	2. 创建一个新的神经网络模型，即 _目标模型_。复制源模型上的模型参数（输出层除外）；输出层的权重采用随机初始化
	3. 向目标模型添加输出层，其输出数是目标数据集中的类别数。然后随机初始化该层的模型参数。
    4. 在目标数据集（如椅子数据集）上训练目标模型。输出层将从头开始进行训练，而所有其他层的参数将根据源模型的参数进行微调。

训练差异：
是一个目标数据集上的正常训练任务，但使用更强的正则化
	使用更小的学习率
	使用更少的数据迭代
源数据集远复杂于目标数据，通常微调的效果更好

固定一些层
神经网络通常学习的是用层次的特征表示
	底层的特征往往更加通用
	高层的特征则更和数据集相关
所以我们可以固定底层一些层的参数，不参与更新--更强的正则

>总结
>微调通过使用在大数据集上得到的预训练效果好的模型来初始化模型权重来完成提升精度
>预训练模型的质量很重要
>微调通常速度更快、精度更高