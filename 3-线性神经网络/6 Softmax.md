# softmax回归  
  
回归用于预测多少的问题。softmax是一种解决分类问题的方法。  
通常解决分类问题，我们关注两点：1、硬性类别：属于哪个类别；2、软性类别：得到属于每个类别的概率。  
  
这两者的界限往往很模糊。其中的一个原因是：即使我们只关心硬类别，我们仍然使用软类别的模型。  
  
统计学家很早以前就发明了一种表示分类数据的简单方法：独热编码（one-hot encoding）。  
独热编码是一个向量，它的分量和类别一样多。 类别对应的分量设置为1，其他所有分量设置为0。  
比如$y$是一个向量，其中$(1,0,0)$对应于“猫”，$(0,1,0)$对应于“狗”。

### 网络架构
前面的回归问题仅有一个数值的输出，而为了估计所有可能类别的条件概率，我们需要一个有多个输出的模型，每个类别对应一个输出。为了解决线性模型的分类问题，我们需要和输出一样多的 **_仿射函数_**（affine function）。 每个输出对应于它自己的仿射函数。

下面举个例子，假设我们有4个特征和3个输出类别，我们将需要12个权重$w$来对其进行表示；3个偏置$b$ .
$$
\begin{align}
o1 = x1w11+x2w12+x3w13+x4w14+b1 \\
o2 = x1w21+x2w22+x3w23+x4w24+b2 \\
o3 = x1w31+x2w32+x3w33+x4w34+b3
\end{align}
$$
我们可以用图来描述这个计算过程。 与线性回归一样，softmax回归也是一个单层神经网络。 
由于计算每个输出𝑜1、𝑜2和𝑜3取决于 所有输入𝑥1、𝑥2、𝑥3和𝑥4， 所以softmax回归的输出层也是全连接层。
![[Pasted image 20250305154907.png || 300]]
从简洁来看，我们可以通过向量表示：$o=Wx+b$ 权重被放入了一个$3×4$的矩阵中。

### 全连接层的参数开销
（全连接就是每个节点都连接到下一层的所有节点）
对于任何具有𝑑个**输入**和𝑞个**输出**的全连接层， 参数开销为𝑂(𝑑𝑞)，这个数字在实践中可能高得令人望而却步。 
幸运的是，将𝑑个输入转换为𝑞个输出的成本可以减少到𝑂(𝑑𝑞/𝑛)， 其中超参数𝑛可以由我们灵活指定。

### softmax计算
我们需要对预测 $o$ 进行规划化，
	一方面，我们希望限制这些输出数字的总和为“1”；
	另一方面，根据不同的输入，我们希望确保其不为负值
$$
\begin{align}
y^{'} = softmax(o) \   其中y^{'}_{j} = exp(o_{j})/\sum exp(o_{k})
\end{align}
$$
这里$y$ 可以视为一个概率分布，在预测过程中，我们可以用下式来选择最有可能的类别。
$$
argmax \ y_{j} = argmax \ o_{j}
$$
尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个 _线性模型_（linear model）。

### 小批量样本的矢量化
为了提高计算效率并且充分利用GPU，我们通常会对小批量样本的数据执行矢量计算。
简单来说小批量样本的矢量化是**将多个样本的运算合并为矩阵/张量操作**的技术，目的是避免 逐样本循环 ，大幅提升计算效率。

### 损失函数
使用最大似然估计
softmax 函数给出了一个向量 $y$ ，其表示**给定任意输入x的每个类的条件概率** 。
例如， $y_1=P(y=猫|x)$ 
$$
P(Y|X) = \prod P(y^{i} | x^{i})
$$
根据最大似然估计，最大化$P(Y|X)$ 相当于**最小化负对数似然**：
$$
-logP(Y|X) = \sum -logP(y_i|x_i) = \sum l(y_i,y'_i)
$$
其中，对任何标签y和模型预测y‘，损失函数为：$l(y_i,y'_i)=- \sum y_jlogy'_j$ ，该损失函数常被称为交叉熵损失。由于y是一个独热编码向量，仅有一个项为1，而y’为预测的概率，其对数不会超过0。